---
title: "Tutorial 6"
author: "Maria, Marc"
date: "12/16/2019- 12/20/2019-"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(cowplot)
library(likelihood)
```

## Overview

- Bootstrapping  
- Probability recap
- Models
    + Formal definition
    + Binomial
    + Normal
- Parameters and Priors
- Frequentists vs. Bayesians

## Bootstrapping
- to do sth *by one's own bootstraps*
    + to do sth by one's own initiative and effort, with little to no assistance

- in statistics
    + idea: we have limited data at hand and we need to put in our own "effort" to get the most out of the data
    + practice: random **re-samples with replacement**


## Bootstrapping    
- specifically for 95% **Confidence Intervals** (e.g. of the mean)
    + vector $D$ with length $k$
    
    <br>
    1. take $k$ samples (**with replacement!**) from $D$, gather them in a vector $D^{rep}$
    2. compute $\mu(D^{rep})$ and add it to the vector $\mu_{sampled}$
    3. repeat 1 and 2
    4. **lower bound** of the 95% CI = **2.5% quantile** of $\mu_{sampled}$
    5. **upper bound** of the 95% CI = **97.5% quantile** of $\mu_{sampled}$


## Probability Recap
- **marginal**: the prob. of one event happening without considering any other events that might play a role
    + e.g. $P(heads) = \frac{1}{2}, \ P(spades) = \frac{1}{4}$

- **joint**: the prob. of many events happening at the same time
    + if *heads* and *spades* are independent: $P(heads, \ spades) = P(heads) \cdot (spades)$

<br>

- **conditional**: the prob. of an event when you know another event already happened
    + $P(A \ | \ B) = \frac{P(A, \ B)}{P(B)}$
    + e.g. $P(heads) = \frac{1}{2}, \ P(heads \ | \ unfair) = \frac{3}{4}$
    
    
## Bayes' Rule
- $P(\theta \ | \ D) = \frac{P(D \ | \ \theta) \cdot P(\theta)}{P(D)}$

- **rewrite** a certain probability in terms of **other probabilities** that might be much easier to compute
- gain insight into the ***unobservable*** causes of any given observation
    + for data analysis: starting from the **raw data**, gain insight into the (parameters of the) **underlying distribution** that generated the data at hand
    
## Bayes' Rule: Homework Questions
- Medical Test?
- Bertrand Box Paradox?

## Models
- A **statistical model** is a simple and elegant mathematical **representation of a random process**
    + captures the essence of the random process and leaves out all the irrelevant details

- A model $M$ consists of
    + a dataset $D$ with **independent variables** $D_{IV}$ and **dependent variables** $D_{DV}$
    + a **likelihood function** $P_M(D_{DV} \ | \ D_{IV}, \theta)$
    + the model's **free parameters** $\theta$ allow you to fine-tune the model to your specific problem
    + **Bayesian models** additionally have a **prior distribution** $P_M(\theta)$


## Likelihood {.bigger}
Likelihood   -  the likeliness of different parameters of a  distribution. In a sence it is a quantitative measure of model fit.

```{r, echo=TRUE}
#likelyhood that p==0.5 for a coin which fall head up 2 times out of 
#4 tosses
lik1<- dbinom(2, size=4, prob=0.5, log=FALSE)

#likelyhood that p==0.5 for a coin which fall head up 4 times out of 
#4 tosses
lik2 <- dbinom(4, size=4, prob=0.5, log=FALSE)
lik1
lik2

```

## Likelihood plot
```{r}
p <- seq(0.01, 1, by=0.01) 
Lik <- numeric(length=100)
for(i in 1:100){
  Lik[i] <- prod(dbinom(2,size=4,prob=p[i]))
}
plot(Lik~p,lty="solid",type="l", xlab="Coin fairness", ylab="Likelihood", main="2 heads in 4 trials")
```

## Likelihood plot

```{r}
p <- seq(0.01, 1, by=0.01) 
Lik <- numeric(length=100)
for(i in 1:100){
  Lik[i] <- prod(dbinom(4,size=4,prob=p[i]))
}
plot(Lik~p,lty="solid",type="l", xlab="Coin fairness", ylab="Likelihood", main="4 heads in 4 trials")
```


## Log likelihood 
Log transformations needed to make working with likelihoods easier!
$\log\prod x_i=log\sum x_i$
Thus, even if you have very small numbers LL does not converge to zero!
```{r}
# plot out the log-likelihood

p <- seq(0.01, 1, by=0.01)
LogLik <- numeric(length=100)
for(i in 1:100){
  LogLik[i] <- sum(dbinom(2, size=4, 
  prob=p[i],log=TRUE))
}
plot(LogLik~p,lty="solid",type="l", xlab="Coin fairness", ylab="Log Likelihood")
```



## Models vs Distributions
- What is the difference between a model and a distribution?
```{r}
# YOUR ANSWER HERE
```

    
## Binomial distribution
- Used to count the number $k$ of successes out of $N$ trials in a **binary** random process
 $$P(A) = \binom{N}{k} \cdot p^k(1-p)^{N-k}$$
 
- Three assumptions:
    + only **two possible outcomes** (success or failure)
    + probability of success is the **same for each trial**
    + the trials are **independent**

Mean: $\mu = np$


Standard Deviation: $\sigma=\sqrt{n(p)(1-p)}$


## Binomial distribution: Example
Each day of a month a student learns 10 new words
Aprox. 10% would be forgotten 
```{r, echo=TRUE}
forgotten <- rbinom(30, 10,.10)
plot(forgotten)
```





## Binomial distribution: Parameters
- Mean: $\mu = np$ and Standard Deviation: $\sigma=\sqrt{n(p)(1-p)}$

```{r}
# Given the plot below, in which way do the the parameters k (number of successes) 
#and N(number of trials) affect the binomial distribution?

sample_size <- 100
x <- 1:sample_size

binomial_dists <- tibble(
    x = x, 
    y_standard = dbinom(x, size = sample_size, prob = 0.5),
    y_low_bias = dbinom(x, size = sample_size, prob = 0.25),
    y_high_bias = dbinom(x, size = sample_size, prob = 0.75),
    y_small_size = dbinom(x, size = 50, prob = 0.5)
)

# What do you expect the geom_lines to look like in the plot?
ggplot(data = binomial_dists, mapping = aes(x = x)) +
    geom_line(mapping = aes(y = y_standard)) +
    geom_line(mapping = aes(y = y_low_bias), colour = "red") +
    geom_line(mapping = aes(y = y_high_bias), colour = "green") +
    geom_line(mapping = aes(y = y_small_size), colour = "blue") +
    xlab("number of successes k") +
    ylab("probability")+
    ggtitle("Effect of parameters on a distribution")
```


## More on Binomial distribution

```{r, echo=TRUE}

dens<- dbinom(30, 300, .1) #probability density distribution   
prob <- pbinom(30, 300, .1) #cumulative probability of an event.
quantile<- qbinom(0.54, 300, .1) # takes the probability value and gives
# a number whose cumulative value matches the probability value
round(dens, 3)
round(prob,3)
quantile
```


## Normal distribution
Likelihood function:

$$P(x) = \frac{1}{\sigma*\sqrt{2\pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}} $$
 
- mean, median and mode of the distribution coincide
- curve of the distribution is symmetrical about the mean.
- total area under the curve is 1 (as for all continuous distributions).

If $\mu=0$ and $\sigma=1$ It is called standart normal distribution

## Normal distribution: Example

```{r, echo=TRUE}
#Imagine we mesure IQ of 1000 people. Then we plot raw data.
IQ <- rnorm(1000, 100, 15)
```

```{r}
mu<- mean(IQ)
median_ <- median(IQ)
plot(IQ, main=paste("Mean:",round(mu,2), "Median:", round(median_,2) )) 
```

## Normal distribution: Density
```{r, echo=TRUE}
dens <-density(IQ)
plot(dens, main = "Density of sampled data")
```


## Normal distribution: Parameters
- Given the plot below, in which way do the the parameters $\mu$ (mean) and $\sigma$ (sd) affect the normal distribution?
```{r}
sample_size <- 1000
x <- seq(-5, 5, length = sample_size)

normal_dists <- tibble(
    x = x, 
    y_standard = dnorm(x, mean = 0, sd = 1),
    y_shifted = dnorm(x, mean = 3.75, sd = 1),
    y_tight = dnorm(x, mean = 0, sd = 0.5),
    y_wide = dnorm(x, mean = 0, sd = 2),
)

# What do you expect the geom_lines to look like in the plot?
ggplot(data = normal_dists, mapping = aes(x = x)) +
    geom_line(mapping = aes(y = y_standard)) +
    geom_line(mapping = aes(y = y_shifted), color = "red") +
    geom_line(mapping = aes(y = y_tight), color = "green") +
    geom_line(mapping = aes(y = y_wide), color = "blue") +
    ylab("probability density")
```



## T-model
- used to compare means of two distrubutions
- both distributions  should come from on family (e.g.normal)
- $\delta$ indicates differencies between the means

```{r, out.width="100%"}
knitr::include_graphics("t-test-model-difference.png")
```







## Which model to use? {.build}

    - if you are intrested how much beer should you buy for a party? 
    
    - if you want to check if male height is different among Norh and South Corenians?
    
    - if you want to check whether a new drink increases sense of humor?


## Priors
- represent the modeller's **subjective** *a priori* beliefs about the random process at hand
- may also just be used **pragmatically** to make calculations and statistical reasoning more elegant

- priors **restrict the range of values** any given parameter can take
    + **strongly informative** prior restrict the range a lot
    + **weakly informative** priors restrict the range only a bit
    + **uninformative** priors do not restrict the range, they assume all parameter values are equally likely
    

## Priors: Exercise
- What types of priors can you identify in the plot below?
```{r}
sample_size <- 100
x <- seq(0, 1, length = sample_size)

beta_dists <- tibble(
    x = x, 
    y_flat = dbeta(x, shape1 = 1, shape2 = 1),
    y_strong = dbeta(x, shape1 = 30, shape2 = 40),
    y_weak = dbeta(x, shape1 = 3, shape2 = 2)
)

# What do these plots represent?
ggplot(data = beta_dists, mapping = aes(x = x)) +
    geom_line(mapping = aes(y = y_flat)) +
    geom_line(mapping = aes(y = y_strong), color = "red") +
    geom_line(mapping = aes(y = y_weak), color = "green") +
    xlab(expression(theta)) +
    ylab(expression(P(theta)))
```

## Priors: example
- For the calculation  how much beer should you buy for a party:
    + our very close friends are coming and we know their habits: **strongly informative**
    + our not that close friends are coming and we have a rough idea of their habits: **weakly informative**
    + many strangers are coming, we dont really know their habits: **uninformative prior**s

    
    
## Frequentists vs. Bayesians

![](./Freq_vs_bias.png)
